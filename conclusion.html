<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <link rel="stylesheet" href="style.css">

  <title></title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body { margin: 0; font-family: sans-serif; background: #121212; color: #fff; }
    header, nav, footer { background: #1f1f1f; padding: 20px; text-align: center; }
    nav a { color: #fff; text-decoration: none; margin: 0 10px; font-weight: bold; }
    section { padding: 40px 20px; max-width: 1000px; margin: auto; }
    .video-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); gap: 40px; }
    .video-box { background: #1e1e1e; border-radius: 10px; padding: 15px; transition: 0.3s ease; }
    .video-box:hover { transform: scale(1.02); }
    video { width: 100%; border-radius: 10px; }
    h2 { border-bottom: 2px solid #555; padding-bottom: 10px; }
  </style>
  
</head>
<body>
  <header><h1>Final Insights & Discussion</h1></header>
 <nav>
    <a href="index.html">Home</a>
    <a href="cifar100.html">CIFAR-100</a>
    <a href="architecture.html">Architecture</a>
    <a href="spectral.html">Spectral</a>
    
    <a href="metrics.html">Metrics</a>
    <a href="conclusion.html">Conclusion</a>
    

  </nav>
  <section>
    <h2>What We Learned</h2>
    <ul>
      <li><strong>Bias Evolution:</strong> Bias distributions reflect how different layers learn at different rates. Shifts in histograms per epoch help detect stabilization.</li>
      <li><strong>Weight Dynamics:</strong> Histogram evolution from peaky (underfitting) to balanced (generalization) to flattened (overfitting) aligns with training progression.</li>
      <li><strong>ESD & Power Law:</strong> ESD plots from SVD reveal structural behaviors — power-law tails imply better generalization; deviation or noise indicates memorization or instability.</li>
      <li><strong>Alpha Trends:</strong> Tracking alpha values shows convergence towards generalization (alpha 2–5) after chaotic early training phases.</li>
    </ul>
  </section>

  <section>
    <h2>Alpha Evolution Summary</h2>
    <p>Power law exponents (alpha) tracked layer-wise show a steep drop during initial epochs, with most settling in a generalization-ideal range of 2–5. Spikes later in training may indicate layer instability or overfitting risks.</p>
    <img src="alpha.png" alt="Alpha Evolution Plot" style="width:100%; border-radius:10px; margin-top:20px;">
  </section>

  <section>
    <h2>Generalized Training Phases</h2>
    <ul>
      <li><strong>Epochs 1–10:</strong> Underfitting stage; weights/biases concentrated near zero, unstable alpha.</li>
      <li><strong>Epochs 10–30:</strong> Generalization sweet spot; stable ESD, smooth bias/weight evolution, alpha stabilizes.</li>
      <li><strong>Epochs 30–49:</strong> Potential overfitting onset; noisy ESD, sudden alpha spikes.</li>
    </ul>
  </section>

  <section>
    <h2>Future Scope</h2>
    <p>Future improvements could involve:</p>
    <ul>
      <li>Layer-wise pruning guided by alpha values and ESD plots</li>
      <li>Advanced regularization like spectral norm constraints</li>
      <li>Latent feature analysis using t-SNE or PCA</li>
      <li>Training-time alpha trend monitoring to automate early stopping</li>
    </ul>
  </section>

  <footer>&copy; 2025 CIFAR100 Neural Network Project</footer>
</body>
</html>

